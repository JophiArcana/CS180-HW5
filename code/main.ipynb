{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QupgVmcZgvyx"
      },
      "outputs": [],
      "source": [
        "## Created by Wentinn Liao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k722vkzdWafR"
      },
      "source": [
        "# CS180 Project 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S27iMYAOuWiB",
        "outputId": "c260dac3-ab30-45d1-e373-183379efe9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xHh9PF6hl3R",
        "outputId": "cc3a7301-a01c-449d-9424-e4e3608b4cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/CS180/HW5\n"
          ]
        }
      ],
      "source": [
        "#@title Symlink Setup\n",
        "def ptpp(PATH: str) -> str: # Converts path to python path\n",
        "    return PATH.replace('\\\\', '')\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/CS180/HW5'\n",
        "if not os.path.exists(ptpp(DRIVE_PATH)):\n",
        "    %mkdir $DRIVE_PATH\n",
        "SYM_PATH = '/content/HW5'\n",
        "if not os.path.exists(ptpp(SYM_PATH)):\n",
        "    !ln -s $DRIVE_PATH $SYM_PATH\n",
        "%cd $SYM_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h7SwLD3uWiC",
        "outputId": "e5b5b68f-f1dd-4ce9-d587-3228e2d30070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (68.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.32.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.1.78)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: viser in /usr/local/lib/python3.10/dist-packages (0.1.10)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.2.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.9.26)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.2)\n",
            "Requirement already satisfied: lazy_loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from viser) (12.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from viser) (1.0.7)\n",
            "Requirement already satisfied: pyliblzfse>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from viser) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from viser) (4.66.1)\n",
            "Requirement already satisfied: tyro>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from viser) (0.5.14)\n",
            "Requirement already satisfied: gdown>=4.6.6 in /usr/local/lib/python3.10/dist-packages (from viser) (4.6.6)\n",
            "Requirement already satisfied: rich>=13.3.3 in /usr/local/lib/python3.10/dist-packages (from viser) (13.6.0)\n",
            "Requirement already satisfied: trimesh>=3.21.7 in /usr/local/lib/python3.10/dist-packages (from viser) (4.0.4)\n",
            "Requirement already satisfied: nodeenv>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from viser) (1.8.0)\n",
            "Requirement already satisfied: psutil>=5.9.5 in /usr/local/lib/python3.10/dist-packages (from viser) (5.9.5)\n",
            "Requirement already satisfied: yourdfpy>=0.0.53 in /usr/local/lib/python3.10/dist-packages (from viser) (0.0.53)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.6.6->viser) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown>=4.6.6->viser) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.6.6->viser) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.6.6->viser) (4.11.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.3->viser) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.3->viser) (2.16.1)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.2.0->viser) (0.15)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.2.0->viser) (4.5.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.2.0->viser) (1.6.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from yourdfpy>=0.0.53->viser) (4.9.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.3->viser) (0.1.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from trimesh>=3.21.7->viser) (6.7.0)\n",
            "Requirement already satisfied: mapbox-earcut in /usr/local/lib/python3.10/dist-packages (from trimesh>=3.21.7->viser) (1.0.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from trimesh>=3.21.7->viser) (5.2.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from trimesh>=3.21.7->viser) (4.19.2)\n",
            "Requirement already satisfied: svg.path in /usr/local/lib/python3.10/dist-packages (from trimesh>=3.21.7->viser) (6.3)\n",
            "Requirement already satisfied: pycollada in /usr/local/lib/python3.10/dist-packages (from trimesh>=3.21.7->viser) (0.7.2)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from trimesh>=3.21.7->viser) (2.0.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from trimesh>=3.21.7->viser) (3.4.1)\n",
            "Requirement already satisfied: rtree in /usr/local/lib/python3.10/dist-packages (from trimesh>=3.21.7->viser) (1.1.0)\n",
            "Requirement already satisfied: embreex in /usr/local/lib/python3.10/dist-packages (from trimesh>=3.21.7->viser) (2.17.7.post3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.6.6->viser) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.6.6->viser) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.6.6->viser) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.6.6->viser) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.6.6->viser) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.6.6->viser) (1.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->trimesh>=3.21.7->viser) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->trimesh>=3.21.7->viser) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->trimesh>=3.21.7->viser) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->trimesh>=3.21.7->viser) (0.12.0)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: tensordict in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (68.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.41.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.27.7)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (17.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensordict) (1.26.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from tensordict) (2.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U setuptools numpy imageio scikit-learn scikit-image opencv-python matplotlib viser\n",
        "!pip install torch==2.0.0 tensordict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9pbdKYaruWiD"
      },
      "outputs": [],
      "source": [
        "#@title Configure Jupyter Notebook\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "uK2SmUy3uWiD",
        "outputId": "c1f1c997-17d2-4312-edf0-970abe258615"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Library Setup\n",
        "import base64\n",
        "import io\n",
        "import itertools\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "import skimage as sk\n",
        "import skimage.io as skio\n",
        "import skimage.transform as sktr\n",
        "import cv2\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Fn\n",
        "import torch.utils as ptu\n",
        "import torch.optim as optim\n",
        "from torchdata.datapipes.map import Zipper, SequenceWrapper\n",
        "from tensordict import TensorDict\n",
        "import tensorflow as tf\n",
        "import json\n",
        "from PIL import Image, ExifTags\n",
        "from typing import *\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "dev_type = 'cuda'\n",
        "\n",
        "plt.axis('off')\n",
        "\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "torch.set_default_dtype(torch.double)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZrE7RRVs886O"
      },
      "outputs": [],
      "source": [
        "#@title Utilities\n",
        "def read_image(imname: str) -> torch.Tensor:\n",
        "    img = Image.open(imname)\n",
        "    return torch.Tensor(sk.img_as_float(np.array(img.convert('RGBA')))) # , {ExifTags.TAGS[k]: v for k, v in img._getexif().items()}\n",
        "\n",
        "def im_rescale(im):\n",
        "    lo = np.min(im)\n",
        "    hi = np.max(im)\n",
        "    return (im - lo) / (hi - lo)\n",
        "\n",
        "def im_saturate(im):\n",
        "    return np.stack([im_rescale(im[:, :, c]) for c in range(im.shape[2])], axis=2)\n",
        "\n",
        "def multiply_outer(v: np.ndarray, arr: np.ndarray, axis=None):\n",
        "    if axis is None:\n",
        "        axis = v.ndim\n",
        "    arr_ = arr.transpose(*range(axis, arr.ndim), *range(axis))\n",
        "    return (arr_ * v).transpose(*range(arr.ndim - axis, arr.ndim), *range(arr.ndim - axis))\n",
        "\n",
        "def plot_cycle(ax, points: np.ndarray, **kwargs):\n",
        "    cycled_points = np.vstack([points, points[:1]])\n",
        "    ax.plot(*cycled_points.T, **kwargs)\n",
        "\n",
        "def show_video(video_name: str):\n",
        "    if os.path.exists(video_name):\n",
        "        video = io.open(video_name, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "            loop controls style=\"height: 400px;\">\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "def color(z: float, scale: float=120.) -> np.ndarray:\n",
        "    k = 2 * np.pi * z / scale\n",
        "    return (1 + np.asarray([np.sin(k), np.sin(k + 2 * np.pi / 3), np.sin(k + 4 * np.pi / 3)], dtype=float)) / 2\n",
        "\n",
        "def psnr(x: torch.Tensor) -> torch.Tensor:\n",
        "    return -10 * torch.log10(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class Timer(object):\n",
        "    indent = 0\n",
        "    p = False\n",
        "\n",
        "    def __init__(self):\n",
        "        self.start_t = 0\n",
        "\n",
        "    def start(self, name):\n",
        "        if Timer.p:\n",
        "            print('\\t' * Timer.indent + name + ' {')\n",
        "            Timer.indent += 1\n",
        "        self.start_t = time.time_ns()\n",
        "\n",
        "    def stop(self):\n",
        "        if Timer.p:\n",
        "            Timer.indent -= 1\n",
        "            print('\\t' * Timer.indent + '} ' + f'{(time.time_ns() - self.start_t) * 1e-6}ms,')\n",
        "        # return time.time_ns() - self.start_t"
      ],
      "metadata": {
        "id": "07tXktty9tHn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1x42pGm7aOF"
      },
      "source": [
        "# Part 1. Fit a Neural Field to a 2D Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QvdHeNyYLFUb"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, L: int) -> None:\n",
        "        super().__init__()\n",
        "        self.coeff = torch.pow(2, torch.arange(L)) * torch.pi\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        t = self.coeff.to(x.device) * x.unsqueeze(-1)\n",
        "        return torch.cat([x, torch.cos(t).view(*x.shape[:-1], -1), torch.sin(t).view(*x.shape[:-1], -1)], dim=-1)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, L: int, hidden_dim: int, num_hidden_layers: int) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            SinusoidalPositionalEmbedding(L),\n",
        "            nn.Linear(4 * L + 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            *itertools.chain(*[\n",
        "                (nn.Linear(hidden_dim, hidden_dim), nn.ReLU()) for _ in range(num_hidden_layers - 1)\n",
        "            ]),\n",
        "            nn.Linear(hidden_dim, 3),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.to(x.device)\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "hBrmhnTGd1Ih"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "def get_dataloader(im: torch.Tensor, batch_size: int, n_iter: int):\n",
        "    H, W = im.shape[:2]\n",
        "\n",
        "    x_meshgrid, y_meshgrid = torch.meshgrid(torch.arange(H), torch.arange(W))\n",
        "    x_dataset, y_dataset = x_meshgrid.flatten(), y_meshgrid.flatten()\n",
        "\n",
        "    xy_dataset = torch.stack([x_dataset, y_dataset], dim=1) / torch.tensor([H, W])\n",
        "    rgb_dataset = im[x_dataset, y_dataset]\n",
        "\n",
        "    train_dataset = Zipper(\n",
        "        SequenceWrapper(xy_dataset),\n",
        "        SequenceWrapper(rgb_dataset)\n",
        "    )\n",
        "    train_sampler = ptu.data.RandomSampler(\n",
        "        train_dataset,\n",
        "        replacement=True,\n",
        "        num_samples=n_iter * batch_size\n",
        "    )\n",
        "\n",
        "    return ptu.data.DataLoader(\n",
        "        train_dataset,\n",
        "        sampler=train_sampler,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "EvuGsqZN1fD4"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "def test(model: nn.Module, im: torch.Tensor):\n",
        "    H, W = im.shape[:2]\n",
        "\n",
        "    return model(torch.stack(\n",
        "        torch.meshgrid(torch.arange(H) / H, torch.arange(W) / W), dim=-1\n",
        "    ).to(dev_type))\n",
        "\n",
        "\n",
        "def train(model: nn.Module, im: torch.Tensor, n_iter: int, imname: str, save_im=False, log_frequency: int=200):\n",
        "    model.to(dev_type)\n",
        "    L = len(model.layers[0].coeff)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-2)\n",
        "    dataloader = get_dataloader(im, 10000, n_iter + 1)\n",
        "\n",
        "    training_loss, validation_loss = [], []\n",
        "    for t, (X, y) in enumerate(dataloader):\n",
        "        with torch.set_grad_enabled(True):\n",
        "            X, y = X.to(dev_type), y.to(dev_type)\n",
        "            loss = Fn.mse_loss(model(X), y)\n",
        "\n",
        "        if t % log_frequency == 0:\n",
        "            with torch.set_grad_enabled(False):\n",
        "                test_im = test(model, im).cpu()\n",
        "                validation_loss.append(valid_loss := Fn.mse_loss(test_im, im))\n",
        "\n",
        "            print(f'Iteration {t}: training loss {loss.item()}, validation loss {valid_loss.item()}')\n",
        "            training_loss.append(loss)\n",
        "\n",
        "            if save_im:\n",
        "                skio.imsave(f'images/singleview/{imname}_it{t}.jpg', sk.img_as_ubyte(test_im), quality=50)\n",
        "                # skio.imsave(f'images/singleview/hparam/{imname}_L{L}_it{t}.jpg', sk.img_as_ubyte(test_im), quality=50)\n",
        "\n",
        "            plt.imshow(test_im)\n",
        "            plt.show()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return torch.stack(training_loss), torch.stack(validation_loss)\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "im = read_image('code/data/fox.jpg')[:, :, :-1]\n",
        "training_loss, validation_loss = train(MLP(10, 256, 3), im, 1000, 'fox', save_im=True)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "D-F8FM_y1KUC",
        "outputId": "3365f624-8d7b-4633-9938-80b470accfcf"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nim = read_image('code/data/fox.jpg')[:, :, :-1]\\ntraining_loss, validation_loss = train(MLP(10, 256, 3), im, 1000, 'fox', save_im=True)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "plt.plot(torch.arange(0, 1001, 200), psnr(validation_loss))\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('PSNR')\n",
        "\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "e-azieob2n8y",
        "outputId": "0e282a01-fb2c-4bdb-c55e-56bbda7dddca"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nplt.plot(torch.arange(0, 1001, 200), psnr(validation_loss))\\nplt.xlabel('Iterations')\\nplt.ylabel('PSNR')\\n\\nplt.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nYT2sMwzhFWm",
        "outputId": "2e27eda7-6fe6-47e3-b627-6eb30e9ab081"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nim = read_image('code/data/mom_genki.jpg')[:, :, :-1]\\nL_list = list(range(2, 16, 2))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\"\"\"\n",
        "im = read_image('code/data/mom_genki.jpg')[:, :, :-1]\n",
        "L_list = list(range(2, 16, 2))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "BVlxLt3BhK_l",
        "outputId": "9d4104ea-2a23-40c4-be39-29f95a0c05d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntraining_loss, validation_loss = zip(*(train(MLP(L, 256, 3), im, 1000, 'mom_genki', save_im=True) for L in L_list))\\ntraining_loss, validation_loss = torch.stack(training_loss), torch.stack(validation_loss)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\"\"\"\n",
        "training_loss, validation_loss = zip(*(train(MLP(L, 256, 3), im, 1000, 'mom_genki', save_im=True) for L in L_list))\n",
        "training_loss, validation_loss = torch.stack(training_loss), torch.stack(validation_loss)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "-0sks0z0mWHe",
        "outputId": "f17cb8e9-ce93-446e-9775-6d30bc969363"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nx = torch.arange(0, 1001, 200)\\ny = psnr(validation_loss)\\n\\nplt.plot(x, y[L_list.index(10)])\\nplt.xlabel('Iterations')\\nplt.ylabel('PSNR')\\n\\nplt.show()\\n\\nfor i, L in enumerate(L_list):\\n    plt.plot(x, y[i], color=plt.cm.inferno(i / len(L_list)), marker='.', label=f'L={L}')\\n\\nplt.xlabel('Iterations')\\nplt.ylabel('PSNR')\\nplt.legend()\\n\\nplt.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\"\"\"\n",
        "x = torch.arange(0, 1001, 200)\n",
        "y = psnr(validation_loss)\n",
        "\n",
        "plt.plot(x, y[L_list.index(10)])\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('PSNR')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "for i, L in enumerate(L_list):\n",
        "    plt.plot(x, y[i], color=plt.cm.inferno(i / len(L_list)), marker='.', label=f'L={L}')\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('PSNR')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Aa-kIwwoyJL"
      },
      "source": [
        "# Part 2. Fit a Neural Radiance Field from Multi-view Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "H4VxXjIfpopp"
      },
      "outputs": [],
      "source": [
        "data = np.load(f\"code/data/lego_200x200.npz\")\n",
        "\n",
        "# Training images: [100, 200, 200, 3]\n",
        "images_train = torch.tensor(data[\"images_train\"] / 255.0)\n",
        "\n",
        "# Cameras for the training images\n",
        "# (camera-to-world transformation matrix): [100, 4, 4]\n",
        "c2ws_train = torch.tensor(data[\"c2ws_train\"])\n",
        "\n",
        "# Validation images:\n",
        "images_val = torch.tensor(data[\"images_val\"] / 255.0)\n",
        "\n",
        "# Cameras for the validation images\n",
        "# (camera-to-world transformation matrix): [10, 4, 4]\n",
        "c2ws_val = torch.tensor(data[\"c2ws_val\"])\n",
        "\n",
        "# Test cameras for novel-view video rendering:\n",
        "# (camera-to-world transformation matrix): [60, 4, 4]\n",
        "c2ws_test = torch.tensor(data[\"c2ws_test\"])\n",
        "\n",
        "# Camera focal length\n",
        "f = float(data[\"focal\"])  # float\n",
        "\n",
        "H, W = images_train.shape[1:3]\n",
        "K = torch.Tensor([\n",
        "    [f, 0, H / 2],\n",
        "    [0, f, W / 2],\n",
        "    [0, 0, 1]\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubOZQHlECO9v"
      },
      "source": [
        "## Part 2.1. Create Rays from Cameras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "vFhxRRQ6CsM2"
      },
      "outputs": [],
      "source": [
        "def transform(\n",
        "    M: torch.Tensor,    # [... x 4 x 4]\n",
        "    x: torch.Tensor     # [... x 3]\n",
        ") -> torch.Tensor:      # [... x 3]\n",
        "    B = x.shape[:-1]\n",
        "    dev = x.device\n",
        "\n",
        "    augmented_x = torch.cat([x, torch.ones((*B, 1), device=dev)], dim=-1).unsqueeze(-1)\n",
        "    return (M.index_select(-2, torch.arange(3, device=dev)) @ augmented_x).squeeze(-1)\n",
        "\n",
        "def pixel_to_camera(\n",
        "    K_invT: torch.Tensor,    # [3 x 3]\n",
        "    uv: torch.Tensor #,   # [... x 2]\n",
        "    # s: torch.Tensor     # [...]\n",
        ") -> torch.Tensor:      # [... x 3]\n",
        "    B = uv.shape[:-1]\n",
        "    dev = uv.device\n",
        "\n",
        "    augmented_uv = torch.cat([uv, torch.ones((*B, 1), device=dev)], dim=-1)\n",
        "    return augmented_uv @ K_invT\n",
        "\n",
        "def pixel_to_ray(\n",
        "    K_invT: torch.Tensor,   # [3 x 3]\n",
        "    c2w: torch.Tensor,      # [... x 4 x 4]\n",
        "    uv: torch.Tensor        # [... x 2]\n",
        ") -> TensorDict:            # [... x 3], [... x 3]\n",
        "    B = torch.broadcast_shapes(uv.shape[:-1], c2w.shape[:-2])\n",
        "    dev = uv.device\n",
        "\n",
        "    r_o = c2w.permute(-2, -1, *range(len(B)))[:3, -1].permute(*range(1, len(B) + 1), 0)\n",
        "    x_c = pixel_to_camera(K_invT, uv) #, torch.ones(B, device=dev))\n",
        "    x_w = transform(c2w, x_c)\n",
        "    r_d = x_w - r_o\n",
        "    r_d /= torch.norm(r_d, dim=-1, keepdim=True)\n",
        "\n",
        "    return TensorDict({\n",
        "        'origin': r_o.expand(*B, 3),\n",
        "        'direction': r_d.expand(*B, 3)\n",
        "    }, batch_size=B, device=dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgW7h8ejPup-"
      },
      "source": [
        "## Part 2.2. Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VvEN2MhjUwCk"
      },
      "outputs": [],
      "source": [
        "def sample_rays(\n",
        "    ims: torch.Tensor,      # [N x H x W x 3]\n",
        "    K_invT: torch.Tensor,   # [3 x 3]\n",
        "    c2ws: torch.Tensor,     # [N x 4 x 4]\n",
        "    num_samples: int        # B\n",
        ") -> TensorDict:            # [B x 3], [B x 3], [B x 3]\n",
        "    T, H, W, _ = ims.shape\n",
        "    dev = ims.device\n",
        "\n",
        "    h_index = torch.randint(0, H, (num_samples // T,), device=dev)\n",
        "    w_index = torch.randint(0, W, (num_samples // T,), device=dev)\n",
        "\n",
        "    c2w = c2ws[:, None]\n",
        "    uv = torch.stack([w_index, h_index], dim=1)[None] + 0.5\n",
        "\n",
        "    result = pixel_to_ray(K_invT, c2w, uv)\n",
        "    result['rgb'] = ims[:, h_index, w_index]\n",
        "    return result\n",
        "\n",
        "def sample_all_rays(\n",
        "    ims: torch.Tensor,      # [N x H x W x 3]\n",
        "    K_invT: torch.Tensor,   # [3 x 3]\n",
        "    c2ws: torch.Tensor,     # [N x 4 x 4]\n",
        ") -> TensorDict:            # [N x H x W x 3], [N x H x W x 3], [N x H x W x 3]\n",
        "    T, H, W, _ = ims.shape\n",
        "    dev = ims.device\n",
        "\n",
        "    h_index, w_index = torch.meshgrid(\n",
        "        torch.arange(H, device=dev),\n",
        "        torch.arange(W, device=dev)\n",
        "    )\n",
        "\n",
        "    c2w = c2ws[:, None, None]\n",
        "    uv = torch.stack([w_index, h_index], dim=-1)[None] + 0.5\n",
        "\n",
        "    result = pixel_to_ray(K_invT, c2w, uv)\n",
        "    result['rgb'] = ims\n",
        "    return result\n",
        "\n",
        "def sample_along_rays(\n",
        "    rays: TensorDict,               # [... x 3], [... x 3], [... x 3]\n",
        "    near: float = 2.0,\n",
        "    far: float = 6.0,\n",
        "    num_samples: int = 64,          # T\n",
        "    perturb: bool=False\n",
        ") -> torch.Tensor:                  # [... x T x 3]\n",
        "    dev = rays.device\n",
        "    B = rays.shape\n",
        "\n",
        "    inc = (far - near) / num_samples\n",
        "    if perturb:\n",
        "        t = torch.linspace(near, far - inc, num_samples, device=dev) + torch.rand(num_samples, device=dev) * inc\n",
        "    else:\n",
        "        t = torch.linspace(near, far - inc, num_samples, device=dev)\n",
        "    t = t.view(*(1 for _ in range(len(B))), num_samples, 1)\n",
        "\n",
        "    return rays['origin'].unsqueeze(-2) + t * rays['direction'].unsqueeze(-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQvPZ9KIj6dI"
      },
      "source": [
        "## Part 2.3. Putting the Dataloading All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "S09gDHQ8muWF"
      },
      "outputs": [],
      "source": [
        "class RayDataset(object):\n",
        "    def __init__(self, ims, K, c2ws, device):\n",
        "        self.ims = ims.to(device)\n",
        "        self.K = K.to(device)\n",
        "        self.K_invT = torch.linalg.inv(self.K.T)\n",
        "        self.c2ws = c2ws.to(device)\n",
        "\n",
        "    def sample_rays(self, num_samples):\n",
        "        return sample_rays(self.ims, self.K_invT, self.c2ws, num_samples)\n",
        "\n",
        "    def sample_all_rays(self):\n",
        "        return sample_all_rays(self.ims, self.K_invT, self.c2ws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "ki0IN9ySj95o",
        "outputId": "ce23e83f-abc5-44ba-a36c-e95469d324f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport viser, time  # pip install viser\\n\\n# --- You Need to Implement These ------\\ndataset = RayDataset(images_train, K, c2ws_train, \\'cpu\\')\\nrays = dataset.sample_rays(100).flatten(0, 1)\\npoints = sample_along_rays(rays, perturb=True)\\nH, W = images_train.shape[1:3]\\n# ---------------------------------------\\n\\nserver = viser.ViserServer(share=True)\\nfor i, (image, c2w) in enumerate(zip(images_train, c2ws_train)):\\n    server.add_camera_frustum(\\n        f\"/cameras/{i}\",\\n        fov=2 * np.arctan2(H / 2, K[0, 0]),\\n        aspect=W / H,\\n        scale=0.15,\\n        wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz,\\n        position=c2w[:3, 3],\\n        image=image.numpy()\\n    )\\nfor i, (o, d) in enumerate(zip(rays[\\'origin\\'], rays[\\'direction\\'])):\\n    server.add_spline_catmull_rom(\\n        f\"/rays/{i}\", positions=np.stack((o, o + d * 6.0)),\\n    )\\nserver.add_point_cloud(\\n    f\"/samples\",\\n    colors=np.zeros_like(points).reshape(-1, 3),\\n    points=points.numpy().reshape(-1, 3),\\n    point_size=0.02,\\n)\\ntime.sleep(1000)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "\"\"\"\n",
        "import viser, time  # pip install viser\n",
        "\n",
        "# --- You Need to Implement These ------\n",
        "dataset = RayDataset(images_train, K, c2ws_train, 'cpu')\n",
        "rays = dataset.sample_rays(100).flatten(0, 1)\n",
        "points = sample_along_rays(rays, perturb=True)\n",
        "H, W = images_train.shape[1:3]\n",
        "# ---------------------------------------\n",
        "\n",
        "server = viser.ViserServer(share=True)\n",
        "for i, (image, c2w) in enumerate(zip(images_train, c2ws_train)):\n",
        "    server.add_camera_frustum(\n",
        "        f\"/cameras/{i}\",\n",
        "        fov=2 * np.arctan2(H / 2, K[0, 0]),\n",
        "        aspect=W / H,\n",
        "        scale=0.15,\n",
        "        wxyz=viser.transforms.SO3.from_matrix(c2w[:3, :3]).wxyz,\n",
        "        position=c2w[:3, 3],\n",
        "        image=image.numpy()\n",
        "    )\n",
        "for i, (o, d) in enumerate(zip(rays['origin'], rays['direction'])):\n",
        "    server.add_spline_catmull_rom(\n",
        "        f\"/rays/{i}\", positions=np.stack((o, o + d * 6.0)),\n",
        "    )\n",
        "server.add_point_cloud(\n",
        "    f\"/samples\",\n",
        "    colors=np.zeros_like(points).reshape(-1, 3),\n",
        "    points=points.numpy().reshape(-1, 3),\n",
        "    point_size=0.02,\n",
        ")\n",
        "time.sleep(1000)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOt7I_ZqnVoM"
      },
      "source": [
        "## Part 2.4. Neural Radiance Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nn34HCo7nYF3"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "class NeRF(nn.Module):\n",
        "    def __init__(self, L: int=10, L_r: int=4) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.SPE = SinusoidalPositionalEmbedding(L)\n",
        "        self.SPE_r = SinusoidalPositionalEmbedding(L_r)\n",
        "\n",
        "        self.x_layers1 = nn.Sequential(\n",
        "            nn.Linear(3 * (2 * L + 1), 256),\n",
        "            nn.ReLU(),\n",
        "            *itertools.chain(*[\n",
        "                (nn.Linear(256, 256), nn.ReLU()) for _ in range(3)\n",
        "            ])\n",
        "        )\n",
        "        self.x_layers2 = nn.Sequential(\n",
        "            nn.Linear(3 * (2 * L + 1) + 256, 256),\n",
        "            *itertools.chain(*[\n",
        "                (nn.ReLU(), nn.Linear(256, 256)) for _ in range(3)\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        self.density_layers = nn.Sequential(\n",
        "            nn.Linear(256, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.rgb_fc = nn.Linear(256, 256)\n",
        "        self.rgb_layers = nn.Sequential(\n",
        "            nn.Linear(3 * (2 * L_r + 1) + 256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 3),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        for layers in [self.x_layers1, self.x_layers2, self.density_layers, self.rgb_layers]:\n",
        "            for lin in layers[::2]:\n",
        "                nn.init.kaiming_normal_(lin.weight)\n",
        "                nn.init.zeros_(lin.bias)\n",
        "        nn.init.constant_(self.density_layers[0].bias, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,    # [... N x 3]\n",
        "        r_d: torch.Tensor   # [... N x 3]\n",
        "    ) -> torch.Tensor:      # [... N x 3]\n",
        "        self.to(x.device)\n",
        "\n",
        "        spe_x = self.SPE(x)\n",
        "        f = self.x_layers2(torch.cat([spe_x, self.x_layers1(spe_x)], dim=-1))\n",
        "\n",
        "        density = self.density_layers(f)\n",
        "        spe_r_d = self.SPE_r(r_d).expand(*(-1 for _ in range(r_d.ndim - 2)), x.shape[-2], -1)\n",
        "        rgb = self.rgb_layers(torch.cat([spe_r_d, self.rgb_fc(f)], dim=-1))\n",
        "\n",
        "        return density, rgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrfMCWvLH1D-"
      },
      "source": [
        "## Part 2.5. Volume Rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "F8kQoOabH0ZU"
      },
      "outputs": [],
      "source": [
        "def volrend(\n",
        "    sigma: torch.Tensor,    # [... x N x 1],\n",
        "    rgb: torch.Tensor,      # [... x N x 3],\n",
        "    step_size: float,\n",
        "    background: torch.Tensor=None\n",
        ") -> torch.Tensor:          # [... x 3]\n",
        "    B = sigma.shape[:-2]\n",
        "    N = sigma.shape[-2]\n",
        "    dev = sigma.device\n",
        "\n",
        "    T = torch.cat([\n",
        "        torch.ones((*B, 1, 1), device=dev),\n",
        "        torch.exp(torch.cumsum(-step_size * sigma.transpose(0, -2)[:-1].transpose(0, -2), dim=-2))\n",
        "    ], dim=-2)\n",
        "\n",
        "    result = torch.sum((T * (1 - torch.exp(-sigma * step_size))) * rgb, dim=-2)\n",
        "    if background is not None:\n",
        "        rem = torch.exp(-step_size * torch.sum(sigma, dim=-2))\n",
        "        result = result + rem * background\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "PmOtSYpVl8SG"
      },
      "outputs": [],
      "source": [
        "def test3d(\n",
        "    model: NeRF,\n",
        "    rays: TensorDict,   # [... x 3]\n",
        "    perturb: bool,\n",
        "    N: int = 64,\n",
        "    near: float = 2.,\n",
        "    far: float = 6.\n",
        ") -> torch.Tensor:\n",
        "    x = sample_along_rays(rays, near=near, far=far, num_samples=N, perturb=perturb)\n",
        "    r_d = rays['direction'].unsqueeze(-2)\n",
        "\n",
        "    return volrend(*model(x, r_d), (far - near) / N)\n",
        "\n",
        "\n",
        "def train3d(model: NeRF, start_iter, end_iter: int, save_dir: str, log_frequency: int=50, save_frequency: int=50):\n",
        "    model.to(dev_type)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
        "    if start_iter > 0:\n",
        "        if os.path.exists(f'{save_dir}/model_it{start_iter}.pt'):\n",
        "            model.load_state_dict(torch.load(f'{save_dir}/model_it{start_iter}.pt'))\n",
        "        if os.path.exists(f'{save_dir}/optim.pt'):\n",
        "            optimizer.load_state_dict(torch.load(f'{save_dir}/optim.pt'))\n",
        "\n",
        "\n",
        "    %mkdir -p $save_dir\n",
        "\n",
        "    # lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.998)\n",
        "\n",
        "    train_dataset = RayDataset(images_train, K, c2ws_train, dev_type)\n",
        "    valid_dataset = RayDataset(images_val, K, c2ws_val, dev_type)\n",
        "\n",
        "    B, N = 8000, 64\n",
        "    near, far = 2., 6.\n",
        "    delta = (far - near) / N\n",
        "\n",
        "    training_loss, validation_loss, ims = [], [], []\n",
        "    for t in range(start_iter, end_iter + 1):\n",
        "        with torch.set_grad_enabled(True):\n",
        "            rays = train_dataset.sample_rays(B)\n",
        "            loss = Fn.mse_loss(test3d(model, rays, True), rays['rgb'])\n",
        "\n",
        "        if t % log_frequency == 0:\n",
        "            # model.cpu()\n",
        "            with torch.set_grad_enabled(False):\n",
        "                i = np.random.randint(len(images_val))\n",
        "                # i = 0\n",
        "                valid_rays = valid_dataset.sample_all_rays()[i]\n",
        "                ims.append(im := render_im(model, valid_rays).cpu())\n",
        "                valid_loss = Fn.mse_loss(im, images_val[i])\n",
        "            # model.cuda()\n",
        "\n",
        "            print(f'Iteration {t}: training loss {loss.item()}, validation loss {valid_loss.item()}')\n",
        "            training_loss.append(loss.item())\n",
        "            validation_loss.append(valid_loss.item())\n",
        "\n",
        "            plt.imshow(torch.cat([im, images_val[i]], dim=1))\n",
        "            plt.show()\n",
        "\n",
        "            del valid_loss, valid_rays\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if t % save_frequency == 0:\n",
        "            torch.save(model.state_dict(), f'{save_dir}/model_it{t}.pt')\n",
        "            torch.save(optimizer.state_dict(), f'{save_dir}/optim.pt')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        del loss, rays\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return torch.tensor(training_loss), torch.tensor(validation_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def render_im(\n",
        "    model: NeRF,\n",
        "    rays: TensorDict,   # [H x W x 3]\n",
        "    background: torch.Tensor=None,\n",
        "    N: int = 64,\n",
        "    near: float = 2.,\n",
        "    far: float = 6.,\n",
        "    k: int=3\n",
        ") -> torch.Tensor:\n",
        "    dev = rays.device\n",
        "\n",
        "    x = sample_along_rays(rays, near=near, far=far, num_samples=N)\n",
        "    r_d = rays['direction'].unsqueeze(-2)\n",
        "\n",
        "    H, W = rays.shape[:2]\n",
        "    h_inc, w_inc = (H + k - 1) // k, (W + k - 1) // k\n",
        "    with torch.set_grad_enabled(False):\n",
        "        result = torch.empty((H, W, 3), dtype=float, device=dev)\n",
        "        for h in range(0, H, h_inc):\n",
        "            h_index = torch.arange(h, min(H, h + h_inc), device=dev)[:, None]\n",
        "            for w in range(0, W, w_inc):\n",
        "                w_index = torch.arange(w, min(W, w + w_inc), device=dev)[None]\n",
        "                result[h_index, w_index] = volrend(*model(\n",
        "                    x[h_index, w_index],\n",
        "                    r_d[h_index, w_index]\n",
        "                ), (far - near) / N, background=background)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "JtjQg_Pec4PQ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lUbx9-MoyqFL",
        "outputId": "00e5e501-3797-4a51-92cd-c2e6958005af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nmodel = NeRF()\\ntraining_loss, validation_loss = train3d(model, 0, 5000, 'model_v100')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "\"\"\"\n",
        "model = NeRF()\n",
        "training_loss, validation_loss = train3d(model, 0, 5000, 'model_v100')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(dir: str, iteration: int):\n",
        "    model = NeRF().cuda()\n",
        "    model.load_state_dict(torch.load(f'{dir}/model_it{iteration}.pt'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "LR1gzvIgl9th"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "valid_dataset = RayDataset(images_val, K, c2ws_val, dev_type)\n",
        "valid_rays = valid_dataset.sample_all_rays()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gZl9n7R9pGU7",
        "outputId": "c334c60d-6630-4c41-c859-7b32dac647a3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nvalid_dataset = RayDataset(images_val, K, c2ws_val, dev_type)\\nvalid_rays = valid_dataset.sample_all_rays()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for it in range(0, 2001, 200):\n",
        "    model = load_model('model_v100', it)\n",
        "    im = render_im(model, valid_rays[0]).cpu()\n",
        "\n",
        "    skio.imsave(f'images/multiview/lego_it{it}.jpg', sk.img_as_ubyte(im), quality=100)\n",
        "\n",
        "    plt.imshow(im)\n",
        "    plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "fpRSb4sQn5hO",
        "outputId": "173267cc-627a-456e-bb82-c47c3e32f86f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor it in range(0, 2001, 200):\\n    model = load_model('model_v100', it)\\n    im = render_im(model, valid_rays[0]).cpu()\\n\\n    skio.imsave(f'images/multiview/lego_it{it}.jpg', sk.img_as_ubyte(im), quality=100)\\n\\n    plt.imshow(im)\\n    plt.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "validation_loss = []\n",
        "for it in range(0, 2001, 100):\n",
        "    print(f'Iteration {it}')\n",
        "    model = load_model('model_v100', it)\n",
        "    ims = torch.stack([render_im(model, valid_rays[i]).cpu() for i in range(len(images_val))])\n",
        "    validation_loss.append(Fn.mse_loss(ims, images_val).item())\n",
        "validation_loss = torch.tensor(validation_loss)\n",
        "validation_psnr = psnr(validation_loss)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "CBeCOQJJpKz1",
        "outputId": "7aff1deb-16b2-4f7f-ed2c-ea4074af257d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nvalidation_loss = []\\nfor it in range(0, 2001, 100):\\n    print(f'Iteration {it}')\\n    model = load_model('model_v100', it)\\n    ims = torch.stack([render_im(model, valid_rays[i]).cpu() for i in range(len(images_val))])\\n    validation_loss.append(Fn.mse_loss(ims, images_val).item())\\nvalidation_loss = torch.tensor(validation_loss)\\nvalidation_psnr = psnr(validation_loss)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "xjMpjFORjJMn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "968b1cec-85e6-4bd7-ea3a-ef193c0656a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nit = torch.arange(0, 2001, 100)\\nplt.plot(it, validation_psnr)\\n\\nplt.xlabel('Iterations')\\nplt.ylabel('PSNR')\\nplt.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "\"\"\"\n",
        "it = torch.arange(0, 2001, 100)\n",
        "plt.plot(it, validation_psnr)\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('PSNR')\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = len(c2ws_test)\n",
        "test_dataset = RayDataset(torch.zeros(T, H, W, 3), K, c2ws_test, dev_type)\n",
        "test_rays = test_dataset.sample_all_rays()\n",
        "model = load_model('model_v100', 5000)"
      ],
      "metadata": {
        "id": "O58_A3WswQ3V"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ims = torch.stack([render_im(model, test_rays[i]).cpu() for i in range(T)])\n",
        "\n",
        "blue = torch.tensor([153, 246, 255], device=dev_type) / 256\n",
        "ims_blue = torch.stack([render_im(model, test_rays[i], background=blue).cpu() for i in range(T)])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iTtFQ2Kmodl5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9a0266c2-e428-44c6-b3c8-f556e154b19f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nims = torch.stack([render_im(model, test_rays[i]).cpu() for i in range(T)])\\n\\nblue = torch.tensor([153, 246, 255], device=dev_type) / 256\\nims_blue = torch.stack([render_im(model, test_rays[i], background=blue).cpu() for i in range(T)])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "video_name = 'images/multiview/lego_it5000.mp4'\n",
        "fps = 20\n",
        "\n",
        "video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (W, H))\n",
        "for im in ims.numpy():\n",
        "    video.write(sk.img_as_ubyte(im[:, :, ::-1]))\n",
        "video.release()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "M-WrajLFse8r",
        "outputId": "7b821404-2303-4de6-eb36-ef44848ca1c7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nvideo_name = 'images/multiview/lego_it5000.mp4'\\nfps = 20\\n\\nvideo = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (W, H))\\nfor im in ims.numpy():\\n    video.write(sk.img_as_ubyte(im[:, :, ::-1]))\\nvideo.release()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "video_name = 'images/multiview/lego_it5000_blue.mp4'\n",
        "fps = 20\n",
        "\n",
        "video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (W, H))\n",
        "for im in ims_blue.numpy():\n",
        "    video.write(sk.img_as_ubyte(im[:, :, ::-1]))\n",
        "video.release()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "GEYD0jUlxJC7",
        "outputId": "c276a9a7-5f72-46eb-edcd-51abb1d04763"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nvideo_name = 'images/multiview/lego_it5000_blue.mp4'\\nfps = 20\\n\\nvideo = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (W, H))\\nfor im in ims_blue.numpy():\\n    video.write(sk.img_as_ubyte(im[:, :, ::-1]))\\nvideo.release()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KKCeSMhMxf0K"
      },
      "execution_count": 77,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}